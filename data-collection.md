Awesome — I’ll put together a detailed end-to-end system for crawling, deduplicating, processing, and labeling a dataset of 100 million video clips with AI-based metadata extraction. I’ll focus on scalable strategies using your massive compute resources (1000 H100 GPUs, 10k high-end CPUs), and design it for high-efficiency processing, automated AI labeling, deduplication (especially for subset/overlap clips), and storage in a searchable, analytics-friendly format.

I’ll break it down into detailed components: video ingestion and crawling, deduplication and uniqueness detection, video processing and feature extraction, AI metadata generation (using multi-modal and vision-language models), database and data lake setup, and best practices for orchestration, versioning, and future scaling.

I’ll get started and update you once the research is ready.

# Scalable End-to-End Video Data Pipeline Design

## Overview and Objectives  
We propose a **highly efficient, scalable end-to-end pipeline** to collect, deduplicate, process, and automatically label a **dataset of 100 million video clips**. This system will leverage **1000 H100 GPUs** and **10,000 high-end CPU cores** (with unlimited storage) to maximize throughput. The design covers the following stages: 

- **Ingestion** – Massive-scale web crawling and downloading of videos from diverse sources.  
- **Deduplication** – Advanced content deduplication to avoid re-processing duplicate clips (including detecting when one video is a subset of another).  
- **Preprocessing** – Efficiently decoding videos, extracting key frames, audio, and metadata (duration, resolution, etc.).  
- **Automated Labeling** – Using AI models to generate rich metadata: captions, objects, actions, scene type, visual style/mood, time of day, camera motion, number of people, audio events, language, and technical info.  
- **Infrastructure & Orchestration** – A distributed pipeline that fully utilizes GPU/CPU resources (minimizing idle time) with modern orchestration (Ray for parallel processing, plus Airflow/Kubernetes for scheduling and deployment).  
- **Storage & Database** – A combined storage solution (blob storage for raw videos, structured DB for metadata, and vector index for embeddings) to enable fast full-text and semantic search over the dataset.  
- **Scalability & Maintainability** – Modular design allowing easy model retraining/versioning and future expansion beyond 100M videos, with optional human-in-the-loop verification on low-confidence outputs.

Below we detail each component of the system with recommended tools and techniques, supported by up-to-date best practices and research.

## 1. Ingestion: Scalable Video Crawling & Downloading  
**Sources and Crawlers:** We begin with a **scalable web ingestion layer** that can pull videos from various online repositories (public video sites, social media, stock libraries, etc.). A fleet of crawling processes (on the 10,000 CPU cores) will handle fetching video URLs, APIs, and downloads in parallel. For example, we can deploy distributed crawler instances using frameworks like **Apache Nutch or Scrapy** for web crawling, and use site-specific APIs (e.g., YouTube API, Vimeo API) where available for efficiency. Each crawler process queues download tasks for videos, reading from lists of URLs or search queries.

**Distributed Downloading:** To handle the sheer volume (100M clips), we partition the work across many nodes. For instance, using **cloud object storage** (S3 or similar), the crawlers can push URLs into a message queue (like Kafka or AWS SQS). Worker processes then consume these and perform downloads using accelerated networking. We ensure high throughput by enabling HTTP/2 and parallel connections for large files, and by avoiding serial bottlenecks. The downloading tasks should be orchestrated such that network bandwidth is saturated but not overcommitted.

**Format Handling:** Videos may come in various formats. The ingestion stage should **normalize container formats** if needed (without full re-encoding yet) – e.g. ensure a standard MP4 container – to simplify downstream processing. All original files are saved to **blob storage** (a distributed file system or object storage cluster) with a unique ID. Metadata such as source URL, file name, and initial content hash (like MD5 or SHA) are recorded in a master index to track provenance.

**Scale and Monitoring:** Using a workflow scheduler like **Apache Airflow** (or Kubernetes CronJobs), we can schedule continuous crawling jobs. Airflow can coordinate different ingestion tasks (for different source sites) as DAGs, ensuring a steady feed of new videos. The system will monitor ingestion rates and adjust the number of crawler instances to maintain a target throughput. With 10k CPUs, we can easily have thousands of threads downloading concurrently. We also implement basic **duplicate URL filtering** and politeness (respect robots.txt, rate limits) at this stage to avoid repeatedly downloading the same video URL.

*By front-loading a robust ingestion layer, we ensure the pipeline always has content to process, while avoiding obvious duplicates and keeping track of source metadata.* 

## 2. Deduplication: Eliminating Duplicate and Subset Clips  
Before heavy processing, the system **deduplicates videos** to avoid wasting computation on the same content twice. This includes detecting exact duplicates, format variations, and cases where one clip is contained within another longer video.

 ([Using computer vision to automate media content deduplication workflows | AWS for M&E Blog](https://aws.amazon.com/blogs/media/using-computer-vision-to-automate-media-content-deduplication-workflows/)) *Illustration of a video deduplication workflow using content embeddings.* In this approach, videos are broken into frames and **embeddings** are generated for each segment (stored in an embeddings database). A dedicated **deduplication engine** then compares these embeddings to identify overlapping content and cluster duplicate files. This method can detect near-duplicates even if they have minor edits (resized, watermarked) or if one video clip is a sub-section embedded within a longer video. It provides a **duplication score** with confidence for each match ([Using computer vision to automate media content deduplication workflows | AWS for M&E Blog](https://aws.amazon.com/blogs/media/using-computer-vision-to-automate-media-content-deduplication-workflows/#:~:text=Keeping%20the%20above%20in%20mind%2C,secure%20as%20our%20customers%20scale)) ([Using computer vision to automate media content deduplication workflows | AWS for M&E Blog](https://aws.amazon.com/blogs/media/using-computer-vision-to-automate-media-content-deduplication-workflows/#:~:text=The%20solution%20follows%20a%20three,detail%20in%20the%20section%20below)). 

**Perceptual Hashing:** As a first pass, we can use **perceptual video hashing** algorithms to quickly flag potential duplicates. Perceptual hashes (like pHash, aHash for images, extended to video) create a compact signature based on the visual content that is robust to minor changes. For example, extracting a few representative frames (or a frame collage) from each video and computing an image pHash can catch identical or nearly identical videos ([Videohash – Perceptual video hashing python package | Hacker News](https://news.ycombinator.com/item?id=28829777#:~:text=News%20news,of%20the%20python%20imagehash%20package)). If two videos have very similar perceptual hashes for a set of key frames, they are likely duplicates. This is a lightweight check to eliminate obvious dupes.

**Content Fingerprinting:** For more robust deduplication (especially detecting subset relationships), we use **content fingerprints** across the video timeline. A standardized method is the **MPEG-7 Video Signature** tool, which generates a sequence of compact signatures for video segments. This technique is designed to detect duplicate or edited clips even within larger videos ([Mpeg 7 video signature tools for content recognition | PPT](https://www.slideshare.net/slideshow/mpeg-7-video-signature-tools-for-content-recognition/26134618#:~:text=The%20document%20discusses%20the%20MPEG,Read%20less)) ([Mpeg 7 video signature tools for content recognition | PPT](https://www.slideshare.net/slideshow/mpeg-7-video-signature-tools-for-content-recognition/26134618#:~:text=Image%3A%20Why%20The%20MPEG,longer%20piece%20of%20video%20content)). Each video’s signature can be compared to others to find partial overlaps. In practice, we can split each video into fixed-length chunks (e.g. 5-second segments or scene cuts) and compute a fingerprint for each chunk. By sliding these across videos, the system can identify if one video’s chunks appear in another video (signifying a subset). The MPEG-7 standard ensures the fingerprints are robust to transcoding or minor edits and supports **partial matching with accurate temporal localization** of the duplicate segment ([Mpeg 7 video signature tools for content recognition | PPT](https://www.slideshare.net/slideshow/mpeg-7-video-signature-tools-for-content-recognition/26134618#:~:text=www)).

**Multimodal Embedding Matching:** We also leverage AI models to compute **embeddings for video and audio** content, enabling semantic duplicate detection. For each video, we generate: (a) a sequence of **visual embeddings** for frames or short clips (using a CNN or transformer model), and (b) an **audio embedding** for the soundtrack (using a model like VGGish or an AudioSet-trained network). These embeddings capture the content in a high-dimensional vector form. Using a nearest-neighbor search (e.g. Faiss or annoy library), we compare embeddings between videos to find those that are extremely similar. This finds duplicates even if file names or formats differ. The AWS media deduplication solution follows a similar approach: it creates frame-level audio and video embeddings and compares them to detect duplicates with high accuracy ([Using computer vision to automate media content deduplication workflows | AWS for M&E Blog](https://aws.amazon.com/blogs/media/using-computer-vision-to-automate-media-content-deduplication-workflows/#:~:text=Keeping%20the%20above%20in%20mind%2C,secure%20as%20our%20customers%20scale)) ([Using computer vision to automate media content deduplication workflows | AWS for M&E Blog](https://aws.amazon.com/blogs/media/using-computer-vision-to-automate-media-content-deduplication-workflows/#:~:text=embeddings%20for%20the%20audio%20and,contains%20essential%20information%20such%20as)). By capturing fine-grained details (like presence of watermarks or logos in a corner ([Using computer vision to automate media content deduplication workflows | AWS for M&E Blog](https://aws.amazon.com/blogs/media/using-computer-vision-to-automate-media-content-deduplication-workflows/#:~:text=video%20and%20audio%20similarity%20technology,secure%20as%20our%20customers%20scale))), the embedding approach can differentiate between true duplicates and slightly modified content.

**Clustering and Indexing:** We maintain a **“fingerprint index”** (using a vector database or LSH) of all processed videos so far. As new videos come in, their signatures/embeddings are queried against this index to find matches above a similarity threshold. If a new video is found to be a duplicate of an existing one, we **skip full processing** of the duplicate. Instead, we record it as a duplicate entry pointing to the original video’s ID. If the new video is a superset (contains an earlier clip), we can choose to still process it but avoid double-counting the overlapping segment. Conversely, if it’s a subset of a longer video already processed, we might skip it or only process the delta. We also handle **exact file duplicates** by simple checksum comparison (any identical file hash is discarded immediately).

**Output:** The deduplication stage outputs a list of unique video IDs to process, along with a mapping of duplicate relationships. Approximately 100M raw videos may collapse to fewer unique contents after removing duplicates (for example, if 5–10% were duplicates, we might only process ~90M unique videos). This saves considerable storage and compute. All decisions are stored (e.g., in a DB table of duplicate mappings, with a “primary_video_id” for each duplicate group).

## 3. Preprocessing: Efficient Frame, Audio, and Metadata Extraction  
Once we have unique videos, the **preprocessing stage** prepares the data for analysis and ensures uniformity: 

- **Video Decoding & Format Standardization:** Each video is decoded using a high-performance library (e.g., **FFmpeg** with hardware acceleration or GPU decoding via NVDEC). We transcode video frames to a consistent format/resolution if needed. For instance, we might scale all frames to a maximum resolution (e.g. 720p or 1080p) to standardize input to the AI models and reduce memory use for very large videos. The NVIDIA NeMo Video pipeline follows a similar step, transcoding clips to a consistent format after splitting ([Petabyte-Scale Video Processing with NVIDIA NeMo Curator on NVIDIA DGX Cloud | NVIDIA Technical Blog](https://developer.nvidia.com/blog/petabyte-scale-video-processing-with-nvidia-nemo-curator-on-nvidia-dgx-cloud/#:~:text=The%20video%20curation%20pipeline%20involves,textual%20descriptions%20and%20video%20content)). With 10k CPU cores, we can dedicate many threads to decoding; optionally some of the 1000 GPUs can also assist in parallel decoding without impacting their main compute (modern GPUs can decode video streams in hardware while the cores do neural network tasks). This ensures fast frame access for the next steps.

- **Scene Segmentation (Shot Detection):** We perform **shot boundary detection** to split videos into semantically coherent clips if the videos are long. A “shot” is a continuous camera sequence; detecting scene changes helps in downstream analysis (so that labeling can be more fine-grained per scene). We can use algorithms like **PySceneDetect** (content difference-based) or an AI-based shot detector. In the NVIDIA pipeline, a *Shot Detector* model is used as the first stage to split videos into clips ([Petabyte-Scale Video Processing with NVIDIA NeMo Curator on NVIDIA DGX Cloud | NVIDIA Technical Blog](https://developer.nvidia.com/blog/petabyte-scale-video-processing-with-nvidia-nemo-curator-on-nvidia-dgx-cloud/#:~:text=53%20Image%3A%20A%20diagram%20starts,with%20the%20annotation%20implying%20that)). For our pipeline, since our dataset is described as 100M “clips”, it’s possible we are mostly dealing with short videos already. But if some sources provide longer videos, we will split them into smaller clips at this stage. Each resulting clip inherits the ID of the parent video with an extra segment index.

- **Key Frame Extraction:** For each clip, we extract **key frames** or a sampling of frames (e.g., one frame per second or every X frames depending on content). Key frames can be the I-frames from the video codec or frames around detected scene changes. These frames will be used for image-based analysis (object detection, scene classification, etc.). By extracting a limited number of frames that best represent the clip’s content, we reduce redundant computations. If fine temporal analysis (like action recognition) is needed, we might also extract a short **video snippet** or optical flow sequence rather than single frames. The preprocessing ensures these frames/snippets are readily accessible in memory or on fast disk (possibly stored temporarily in a **Ray object store** or a shared memory space for the pipeline).

- **Audio Extraction:** The audio track of each video is extracted as a separate file or buffer (using FFmpeg). We can downsample or convert it to a standard format (e.g., 16 kHz WAV) for uniform processing. If the video has no audio, we mark it accordingly. This audio data will be fed into audio analysis models (for sound event detection or speech-to-text). 

- **Metadata Collection:** We gather **technical metadata** from each file: duration (clip length), frame rate (fps), resolution (width×height), aspect ratio, video codec, audio codec, bit rate, file size, etc. This uses negligible compute (via FFprobe or similar). These technical attributes will be stored in the database and are part of the label set for each video (useful for filtering content by properties like length or quality). They are essentially free labels that come from file headers.

- **Quality Filtering:** As an optional sub-step, we can apply some **quality checks** to filter out unusable data. For example, detect if a video is too short (few frames), has very low resolution, or is a blank screen throughout. Low-quality or corrupt videos can be flagged or removed to avoid wasting resources. This echoes the “Filtering” stage in NVIDIA’s pipeline where models check motion and quality ([Petabyte-Scale Video Processing with NVIDIA NeMo Curator on NVIDIA DGX Cloud | NVIDIA Technical Blog](https://developer.nvidia.com/blog/petabyte-scale-video-processing-with-nvidia-nemo-curator-on-nvidia-dgx-cloud/#:~:text=%E2%80%9Cshot%E2%80%9D%20as%20in%20%E2%80%9Cscene%E2%80%9D%20from,stages%20of%20NeMo%20Video%20Curator)). Simple metrics like average brightness (to catch all-black videos) or checking for identical consecutive frames (to catch static videos) can be used. We might also detect if a video is just a **duplicate frame slideshow** or has large letterbox borders, etc., depending on requirements.

After preprocessing, each video clip is standardized (decoded frames, extracted audio, known metadata) and ready for content analysis. The data is typically kept in memory or fast storage for the next stage. By performing these operations in bulk on CPU (with GPU acceleration for decode where possible), we ensure the expensive GPU models in the labeling stage are not I/O-bound. 

**Throughput Consideration:** With 100M clips, decoding and extracting frames is heavy – but our compute is massive. We would parallelize this by processing many videos concurrently. For example, using **Ray** (a distributed execution framework) we can have a pool of CPU workers dedicated to decoding tasks ([Petabyte-Scale Video Processing with NVIDIA NeMo Curator on NVIDIA DGX Cloud | NVIDIA Technical Blog](https://developer.nvidia.com/blog/petabyte-scale-video-processing-with-nvidia-nemo-curator-on-nvidia-dgx-cloud/#:~:text=For%20each%20stage%2C%20the%20main,with%20the%20actor%20pool%20size)). Ray’s object store could hold the extracted frames in RAM, avoiding writing them to disk between stages ([Petabyte-Scale Video Processing with NVIDIA NeMo Curator on NVIDIA DGX Cloud | NVIDIA Technical Blog](https://developer.nvidia.com/blog/petabyte-scale-video-processing-with-nvidia-nemo-curator-on-nvidia-dgx-cloud/#:~:text=One%20thing%20worth%20mentioning%20is,input%20queue%20of%20stage%20N%2B1)). This zero-copy handoff to the GPU stage ensures we don’t hit storage bottlenecks. The pipeline is designed such that while some videos are being decoded, others are simultaneously being processed by GPUs (overlapping CPU and GPU work).

## 4. Automated Labeling and Annotation  
In this stage, we run a suite of **AI models** on the preprocessed clips to automatically generate a rich set of annotations for each video. We cover multiple aspects of the content to create a comprehensive metadata record. The goal is to produce labels that enable powerful search and downstream ML training on this 100M video dataset, without human intervention except for quality checks. Below are the labeling components and the recommended model types for each:

- **Video Caption (Description):** For each clip, generate a concise **natural-language description**. We use a **vision-language transformer** model trained for video captioning. Modern approaches treat this as a sequence-to-sequence problem: the model ingests video frames (often encoded by a CNN or ViT backbone plus temporal encoder) and outputs a descriptive sentence. For example, **Google’s Vid2Seq model** (2023) is a transformer-based visual-language model pre-trained on millions of narrated videos, capable of producing accurate captions for multiple events in a video ([Vid2Seq: a pretrained visual language model for describing multi-event videos](https://research.google/blog/vid2seq-a-pretrained-visual-language-model-for-describing-multi-event-videos/#:~:text=model%20ai,video%20paragraph%20captioning%20task%2C%20and)). Another approach is to use an image captioning model (like OpenAI’s CLIP with a text generator or BLIP-2) on key frames and then fuse the information. We prefer an end-to-end video captioner for temporal coherence. The caption might describe the main objects and action (e.g., “A person rides a bicycle down a mountain trail on a sunny day”). This provides a high-level summary for search and understanding.

- **Objects Present:** Identify **objects, people, and other entities** visible in the video. We run an **object detection** model on representative frames (and possibly multiple frames to catch all objects). A state-of-the-art detector like **YOLOv8 or DETR (Detection Transformer)** can be used to get bounding boxes and labels for objects ([SlowFast Networks for Video Recognition](https://openaccess.thecvf.com/content_ICCV_2019/papers/Feichtenhofer_SlowFast_Networks_for_Video_Recognition_ICCV_2019_paper.pdf#:~:text=We%20present%20SlowFast%20networks%20for,pointed)). We aggregate the unique object classes present in the clip (e.g., “person, bicycle, tree, dog”). This yields a list of tags for items in the scene. For people, we can also tag attributes like “male adult, female child” if needed (though not identity, just counts and generic attributes for privacy). This step ensures we know every notable object in the video (useful for filtering by content, e.g. find all videos containing “car” or “cat”).

- **Actions and Activities:** Recognize **actions or events** taking place. We use a **video action recognition** model on the clip (or on short sub-clips if needed). For example, the **SlowFast network** by Facebook/Meta is a two-pathway CNN that achieves state-of-the-art accuracy in action classification on video benchmarks ([SlowFast Networks for Video Recognition](https://openaccess.thecvf.com/content_ICCV_2019/papers/Feichtenhofer_SlowFast_Networks_for_Video_Recognition_ICCV_2019_paper.pdf#:~:text=We%20present%20SlowFast%20networks%20for,pointed)). Such models can classify actions like “riding a bike”, “cooking”, “playing guitar”, “swimming”, etc. Newer transformer-based video models (ViViT, VideoSwin, or TimeSformer) could also be employed ([Vid2Seq: a pretrained visual language model for describing multi-event videos](https://research.google/blog/vid2seq-a-pretrained-visual-language-model-for-describing-multi-event-videos/#:~:text=A%20visual%20language%20model%20for,dense%20video%20captioning)). We likely fine-tune on an action taxonomy (like the Kinetics dataset’s 400/600 classes). The output is one or multiple verbs or action labels for the clip. If multiple distinct actions occur, the model or pipeline can list the top ones. This component is crucial for understanding dynamic content beyond static objects.

- **Scene Type (Environment):** Classify the **scene or setting** of the video. We use a scene recognition model (often a CNN trained on scene categories, such as **Places365** dataset) to label the environment: e.g. “outdoor, beach” or “indoor, kitchen” or “office, meeting room”. This gives context of where the video takes place. We can also include whether it’s **indoors vs outdoors**, **urban vs nature**, etc. Scene type can sometimes be inferred from object context, but a dedicated classifier ensures accuracy (e.g., detecting a “desert” scene vs “snowy mountain”). If needed, we can also tag the **genre or category** of the scene (sports field, concert stage, etc. – depending on taxonomy). 

- **Visual Style & Mood:** Analyze the overall **visual style, color tone, and mood** of the video. This is somewhat subjective, but we can approximate it with a combination of analysis and models:
  - Determine if the video is **animated, CGI, or real-life**. (An image classifier can detect cartoon vs real content by texture.)
  - Analyze color distribution to classify tone: e.g., “bright/vibrant colors” vs “dark/monochrome” vs “sepia”. We can use simple color histograms or feed a frame into a small CNN trained for scene mood (some research works classify images as having moods like cheerful, gloomy, warm).
  - **Mood/Aesthetic**: Possibly use an **aesthetic scoring model** or one trained on film styles to label if the scene looks “dramatic, warm, cold, tense, calm,” etc. This is more experimental; alternatively, we tag obvious stylistic aspects (e.g., “black-and-white film” if detected, or “handheld amateur footage” vs “professional cinematic”). 
  - If needed, the captioning model might also hint at mood (e.g., “on a sunny day” implies bright mood). We can augment that or have a separate classifier for weather/time (sunny, rainy, nighttime which affects mood).

- **Time of Day & Lighting:** Using visual cues, classify the **time of day** (day, night, dawn, dusk, etc.) and **lighting conditions** in the scene. Many image models can distinguish day vs night. We could train a simple classifier on frames for categories: daylight, nighttime (dark), indoor artificial lighting, etc. This attribute is useful for searches like “nighttime city footage” or filtering by lighting. If the video is indoors, “well-lit vs dimly lit” could be noted. Time of day might also be inferred from metadata (some videos have timestamp info), but generally visual analysis is sufficient (e.g., presence of sunlight, sky color for dawn/dusk).

- **Camera Motion & Shot Type:** Analyze how the camera is moving and the type of shot:
  - **Camera Motion:** We determine if the camera is static (on a tripod) or moving. If moving, is it panning, tilting, or tracking a subject? We can use **optical flow** analysis across frames: a static camera will have background consistency, whereas a moving camera causes global motion vectors. There are algorithms to classify camera motion patterns (e.g., detect dominant motion vectors for pan/tilt or use gyroscope metadata if available). We can label the video as “static camera”, “slow pan”, “fast jittery (handheld)”, etc. This can be done by simple heuristics on motion magnitude or by training a small model on known camera moves.
  - **Shot Type:** Determine the framing – e.g., “close-up”, “medium shot”, “wide shot/landscape”. We can infer this by detecting how large people or key objects appear (face size relative to frame can indicate close-up vs long shot). If no people, we can use field-of-view heuristics from the scene. This might be a rule-based annotation or a learned one from film data. Also, if it’s an aerial drone footage vs ground-level, that could be noted (maybe via metadata or analyzing perspective). 
  - **Motion Speed:** Additionally, categorize the overall motion in the scene as “slow-paced” or “fast-paced”. This can be based on the amount of movement between frames – e.g., high-action scenes have lots of change, whereas static scenes do not. Optical flow average magnitude could be thresholded for this.

- **Number of People:** Estimate how many **people** are present in the video (and possibly whether they are speaking). Using the results of object detection (or a dedicated person detection model), count the distinct persons visible in the key frames. We might run a **person detector or face detector** on several frames and take the maximum count seen. If faces are detectable, we also confirm they are humans (to avoid counting statues or pictures). This gives a rough count: e.g., “0” (no people), “1 person”, “2-5 people”, “crowd (10+)”. We don’t identify individuals (to keep it privacy-safe), just count. We also note if the person is a **speaker** (if mouth moving with audio speech) or if there’s **dialogue** (this might be gleaned from audio analysis). The number of people is useful for queries like “videos with no people” (for only landscapes etc.) or “group of people”.

- **Audio Analysis (Sounds & Language):** Analyze the **audio track** for two things: sound events and spoken content.
  - **Sound/Audio Events:** Use an **audio classification model** (trained on a broad sound ontology like **AudioSet** which has 632 sound classes ([AudioSet - A sound vocabulary and dataset - Google Research](https://research.google.com/audioset/#:~:text=Research%20research,second))) to label the types of sounds present. This could yield tags like “music”, “applause”, “engine noise”, “bird chirping”, “gunshot”, etc., depending on the clip. Google’s AudioSet provides pretrained models (e.g., VGGish) that turn audio into embeddings which a classifier can map to sound labels ([models/research/audioset/vggish/README.md at master - GitHub](https://github.com/tensorflow/models/blob/master/research/audioset/vggish/README.md#:~:text=VGGish%20converts%20audio%20input%20features,to%20a%20downstream%20classification%20model)). We can run such a model on the audio (possibly in 10-second chunks if audio is long, then aggregate unique sound labels). The output might be a set of tags with confidence, e.g. {“speech”: 95%, “classical music”: 80%}.
  - **Speech Transcription & Language ID:** If the audio contains speech, we use an **automatic speech recognition (ASR)** model to transcribe it or at least to detect the language spoken. For instance, **OpenAI Whisper** or Google’s speech API could be used (Whisper can run on GPUs for transcription). Given the scale, full transcription for 100M videos might be expensive, so we could do a lighter approach: run **language identification** first (models that classify the language from an audio clip) and maybe transcribe only if needed. But since we have a lot of compute, we could transcribe at least a snippet of speech from each video to get an idea of content. The transcript (or keywords from it) can be stored as metadata, useful for search by dialogue. Language detection ensures we know if a video is e.g. in English, Spanish, etc., which can be a label. For non-speech videos, we mark “no speech”. 
  - **Audio Mood:** Additionally, classify the overall audio type: e.g., “music video” (if predominantly music), “narration”, “silence”, etc. This can be derived from the sound tags and volume. For example, if music tag is present and speech is not, tag it as music-only.

- **Aggregating Technical Metadata:** Lastly, we attach the **technical parameters** collected earlier as part of the label set: duration (in seconds), frame rate, resolution, aspect ratio, file size, encoding format. These are stored in the structured database. This allows queries like “find videos under 10 seconds” or “find 4K resolution videos” easily via metadata filters, and also helps in analysis (like distribution of video lengths in the dataset). 

All these labels and annotations are packaged together as the **metadata record** for each video clip. For efficiency, many of these models can run in parallel on the GPU cluster. We would use the 1000 H100 GPUs to run inference on batches of frames or clips concurrently. For example, some GPUs might run the object detection nets while others run the captioning model, etc. However, an emerging trend is to use **multitask models or unified models** to reduce the number of passes: e.g., a single transformer model could potentially output captions, detect objects, and recognize actions if trained appropriately. In practice, we might use a combination: a large multimodal transformer for captioning and maybe object tags (some models can generate captions that include object names), and specialized models for things like face count or audio events that are out-of-scope for a vision-language model.

**Confidence and Human Verification:** Each model provides confidence scores for its predictions. We establish thresholds – if a model is not confident (e.g., caption model is unsure or produced low-likelihood output, or ASR had low confidence), we flag that video for **human review**. The system can route such cases (<5% of videos, for example) into a manual verification queue. Human annotators would then review the video and correct or fill in labels as needed. This ensures quality control on the automated labels. Over time, these human-verified examples can be fed back to improve the models (retraining on mistakes).

## 5. Infrastructure and Orchestration  
To coordinate the above stages on 1000 GPUs and 10k CPUs efficiently, we design a robust orchestration framework. The key goals are to **maximize hardware utilization (avoid idle GPUs)**, handle **heterogeneous workloads** (CPU vs GPU heavy tasks), and allow the pipeline to scale out or in as needed. Modern tools like **Ray**, **Kubernetes**, and workflow managers are utilized here.

 ([Petabyte-Scale Video Processing with NVIDIA NeMo Curator on NVIDIA DGX Cloud | NVIDIA Technical Blog](https://developer.nvidia.com/blog/petabyte-scale-video-processing-with-nvidia-nemo-curator-on-nvidia-dgx-cloud/)) *High-level pipeline stages and compute allocation for video processing.* Raw videos are first downloaded and decoded on CPU (and optionally split into scenes via a “Shot Detector” model). Next, they are transcoded to a standard format. A filtering stage on CPU/GPU applies quality checks and simple metadata extraction (e.g., motion, presence of text overlay, video type). Then, an annotation stage on GPUs runs the heavy AI models (captioning, object recognition – labeled as “Cosmos” and “Nemotron” in one design) to produce textual metadata. Finally, the system computes **embeddings** for both video content and text (on GPU) and outputs these along with the annotated metadata as the processed result ([Petabyte-Scale Video Processing with NVIDIA NeMo Curator on NVIDIA DGX Cloud | NVIDIA Technical Blog](https://developer.nvidia.com/blog/petabyte-scale-video-processing-with-nvidia-nemo-curator-on-nvidia-dgx-cloud/#:~:text=The%20video%20curation%20pipeline%20involves,textual%20descriptions%20and%20video%20content)). This *staged pipeline* allows separate scaling of each component and keeps data in memory between stages for speed ([Petabyte-Scale Video Processing with NVIDIA NeMo Curator on NVIDIA DGX Cloud | NVIDIA Technical Blog](https://developer.nvidia.com/blog/petabyte-scale-video-processing-with-nvidia-nemo-curator-on-nvidia-dgx-cloud/#:~:text=Image%3A%20A%20diagram%20shows%20a,in%20memory%20between%20the%20stages)). 

**Streaming Pipeline with Ray:** We implement the pipeline as a **streaming dataflow** rather than a classic batch sequence. Using **Ray**, we create multiple stages as parallel actors (worker pools) that pass data along via in-memory object stores. For example, Stage 1 = ingestion/decoding, Stage 2 = preprocessing, Stage 3 = AI labeling, Stage 4 = indexing/storage. Each stage can have many workers. Ray’s scheduler can keep the pipeline flowing without writing intermediate results to disk – instead, it passes references to data in the object store to the next stage ([Petabyte-Scale Video Processing with NVIDIA NeMo Curator on NVIDIA DGX Cloud | NVIDIA Technical Blog](https://developer.nvidia.com/blog/petabyte-scale-video-processing-with-nvidia-nemo-curator-on-nvidia-dgx-cloud/#:~:text=One%20thing%20worth%20mentioning%20is,input%20queue%20of%20stage%20N%2B1)). This drastically reduces I/O latency and keeps GPUs busy as soon as data is ready, rather than waiting for entire batches ([Petabyte-Scale Video Processing with NVIDIA NeMo Curator on NVIDIA DGX Cloud | NVIDIA Technical Blog](https://developer.nvidia.com/blog/petabyte-scale-video-processing-with-nvidia-nemo-curator-on-nvidia-dgx-cloud/#:~:text=In%20contrast%2C%20streaming%20processing%20directly,the%20previous%20stages%20are%20complete)) ([Petabyte-Scale Video Processing with NVIDIA NeMo Curator on NVIDIA DGX Cloud | NVIDIA Technical Blog](https://developer.nvidia.com/blog/petabyte-scale-video-processing-with-nvidia-nemo-curator-on-nvidia-dgx-cloud/#:~:text=The%20challenge%20here%20is%20the,primary%20workloads)). Figure 2 in NVIDIA’s blog shows how streaming avoids idle gaps between CPU/GPU steps by overlapping them in time ([Petabyte-Scale Video Processing with NVIDIA NeMo Curator on NVIDIA DGX Cloud | NVIDIA Technical Blog](https://developer.nvidia.com/blog/petabyte-scale-video-processing-with-nvidia-nemo-curator-on-nvidia-dgx-cloud/#:~:text=Image%3A%20A%20diagram%20shows%20a,in%20memory%20between%20the%20stages)).

**Auto-Scaling and Load Balancing:** Each stage in the pipeline might have different throughput (e.g., decoding might be faster or slower than labeling). To prevent bottlenecks, we dynamically adjust the number of workers per stage. The orchestration monitors the queue lengths or throughput of each stage continuously ([Petabyte-Scale Video Processing with NVIDIA NeMo Curator on NVIDIA DGX Cloud | NVIDIA Technical Blog](https://developer.nvidia.com/blog/petabyte-scale-video-processing-with-nvidia-nemo-curator-on-nvidia-dgx-cloud/#:~:text=The%20orchestration%20thread%20also%20manages,overall%20throughput%20of%20each%20stage)). If the GPU stage (say labeling) is slower than the CPU stage feeding it, a queue will build up; the orchestrator can then allocate more GPU workers (if available) for that stage or slow down the feeder. Conversely, if GPUs are waiting idle because the CPU stage is slow (e.g., download lagging), we can increase CPU workers. Using Ray’s ability to easily scale actor pools, we tune the parallelism. NVIDIA’s pipeline scales actor pools at ~3 minute intervals based on throughput measurements, to keep all stages balanced ([Petabyte-Scale Video Processing with NVIDIA NeMo Curator on NVIDIA DGX Cloud | NVIDIA Technical Blog](https://developer.nvidia.com/blog/petabyte-scale-video-processing-with-nvidia-nemo-curator-on-nvidia-dgx-cloud/#:~:text=measure%20the%20overall%20throughput%20of,each%20stage)). For instance, if decoding is twice as fast as labeling, we run twice as many labeling workers (on different GPUs) to match it ([Petabyte-Scale Video Processing with NVIDIA NeMo Curator on NVIDIA DGX Cloud | NVIDIA Technical Blog](https://developer.nvidia.com/blog/petabyte-scale-video-processing-with-nvidia-nemo-curator-on-nvidia-dgx-cloud/#:~:text=In%20the%20simplest%20case%2C%20if,parallel%20manner)). This **auto-scaling** ensures no group of GPUs sits idle waiting for another stage.

**Separation of CPU and GPU Work:** We architect **distinct stages for CPU-heavy vs GPU-heavy tasks**. This was a key insight from similar large-scale pipelines ([Petabyte-Scale Video Processing with NVIDIA NeMo Curator on NVIDIA DGX Cloud | NVIDIA Technical Blog](https://developer.nvidia.com/blog/petabyte-scale-video-processing-with-nvidia-nemo-curator-on-nvidia-dgx-cloud/#:~:text=Separating%20CPU)) ([Petabyte-Scale Video Processing with NVIDIA NeMo Curator on NVIDIA DGX Cloud | NVIDIA Technical Blog](https://developer.nvidia.com/blog/petabyte-scale-video-processing-with-nvidia-nemo-curator-on-nvidia-dgx-cloud/#:~:text=In%20contrast%2C%20you%20have%20more,while%20the%20other%20is%20overloaded)). For example, downloading and preprocessing are mostly CPU-bound (I/O, decoding), whereas inference (labeling) is GPU-bound. By separating them, we can scale each stage with the appropriate resources (we have many more CPU cores than GPUs). CPU stages might run on dozens of nodes, while GPU stages run on the 1000 GPUs. If they were combined, we’d have to assign CPU and GPU to the same task in fixed ratios, which could leave some underutilized ([Petabyte-Scale Video Processing with NVIDIA NeMo Curator on NVIDIA DGX Cloud | NVIDIA Technical Blog](https://developer.nvidia.com/blog/petabyte-scale-video-processing-with-nvidia-nemo-curator-on-nvidia-dgx-cloud/#:~:text=A%20helpful%20way%20to%20think,resources%20don%E2%80%99t%20match%20that%20ratio)). Instead, we keep them decoupled and use a queue to buffer data between them, so both can be busy. In practice, we might even run two sub-stages on GPU: e.g., one for heavy vision models and one for final embedding generation, if those have different performance, and allocate GPUs accordingly.

**Batching and Parallelism:** Within each stage, we also use **batch processing** on the model inference to maximize GPU efficiency. For instance, the object detector can process, say, 16 frames in a batch on one GPU, while the captioning model might process 8 videos in parallel if it’s a per-video model. The pipeline manager can accumulate tasks into batches up to an optimal size before sending to a GPU. This amortizes overhead and utilizes tensor cores fully. We have to keep an eye on latency though – overly large batches add delay. Since throughput is the priority (100M videos), we likely favor near-maximum batch sizes for efficiency, given the data-parallel nature.

**Orchestration Tools:** We deploy this pipeline on a **Kubernetes cluster** for manageability. Each stage’s workers can be containerized (e.g., a container for the decoder, one for each model, etc.). Ray itself can run on Kubernetes, elastically managing pods for workers. We can also integrate Airflow for the higher-level orchestration (like kicking off the whole pipeline job, or scheduling different data source ingestions). However, the *streaming data pipeline* will primarily be controlled by Ray because of its need for low-latency handoffs and dynamic scaling. Airflow could trigger a Ray workflow and then monitor it. Another benefit of K8s is that if we need to scale beyond our current hardware (adding more GPU nodes), the system can accommodate that – it’s cloud-ready. 

**Minimizing Idle GPUs:** The combination of streaming design and auto-scaling essentially keeps GPUs working nearly 100% of the time. In a traditional batch pipeline, GPUs might wait while CPUs do their part on an entire batch ([Petabyte-Scale Video Processing with NVIDIA NeMo Curator on NVIDIA DGX Cloud | NVIDIA Technical Blog](https://developer.nvidia.com/blog/petabyte-scale-video-processing-with-nvidia-nemo-curator-on-nvidia-dgx-cloud/#:~:text=Traditional%20fixed,development%20and%20drive%20up%20costs)) ([Petabyte-Scale Video Processing with NVIDIA NeMo Curator on NVIDIA DGX Cloud | NVIDIA Technical Blog](https://developer.nvidia.com/blog/petabyte-scale-video-processing-with-nvidia-nemo-curator-on-nvidia-dgx-cloud/#:~:text=Traditional%20data%20processing%20often%20relies,one%20stage%20at%20a%20time)). Here, as soon as one video is decoded, it can be sent to a GPU for labeling, overlapping compute and I/O ([Petabyte-Scale Video Processing with NVIDIA NeMo Curator on NVIDIA DGX Cloud | NVIDIA Technical Blog](https://developer.nvidia.com/blog/petabyte-scale-video-processing-with-nvidia-nemo-curator-on-nvidia-dgx-cloud/#:~:text=In%20contrast%2C%20streaming%20processing%20directly,the%20previous%20stages%20are%20complete)). We also **over-provision CPU workers** relative to GPU, as recommended ([Petabyte-Scale Video Processing with NVIDIA NeMo Curator on NVIDIA DGX Cloud | NVIDIA Technical Blog](https://developer.nvidia.com/blog/petabyte-scale-video-processing-with-nvidia-nemo-curator-on-nvidia-dgx-cloud/#:~:text=Based%20on%20the%20relative%20throughput%2C,is%20fluctuation%20in%20the%20pipeline)): e.g., spin up more CPU decoding jobs than strictly needed, so that there’s always decoded frames ready in the queue for GPUs. The 10k CPUs are plenty to ensure a steady supply of data. If any GPU does go idle momentarily, the orchestrator will detect that and can assign it a new task immediately from the queue. We will also prioritize critical path tasks on GPUs – e.g., perhaps do decoding on CPU exclusively to free GPUs only for ML inference.

**Human-in-the-Loop Interface:** We integrate a **human verification interface** for flagged videos (low-confidence cases). This can be a simple web dashboard listing videos needing review, with the current labels pre-filled and an easy way for a human to accept or correct them. We might use an annotation tool or even outsource to a service (if the volume is not huge). These corrections then get fed into the system (the database is updated with the human-verified labels). The pipeline can run continuously, and humans check a small subset in parallel, effectively implementing quality control without stopping the pipeline.

**Throughput Expectations:** With 1000 H100 GPUs, our system is extremely powerful. For perspective, NVIDIA’s internal pipeline processed **1 million hours of video in one day using 2000 H100 GPUs** ([Nvidia just released a large-scale video curation pipeline at GTC built… | Anyscale](https://www.linkedin.com/posts/joinanyscale_nvidia-just-released-a-large-scale-video-activity-7308169936755933184-neGq#:~:text=Ray,in%2FeCe37rsG)). Our task (100M clips) could be of similar order (depending on average clip length). We expect on the order of tens of thousands of videos processed per hour. By tuning the pipeline, we aim to maximize throughput and scale linearly with additional resources if needed.

## 6. Storage and Database Architecture  
After processing and labeling, we need to **store the resulting dataset** (100M videos + metadata) in a way that is queryable and supports both text search and semantic (vector) search. We design a multi-tier storage system:

- **Blob Storage for Videos:** All raw video files and their derived clips are stored in a distributed object store (e.g., HDFS, Cloud Object Storage, or Ceph). This provides **unlimited storage** and replication for durability. Each video is identified by a unique ID (or a content hash) which corresponds to the file path. We might partition the storage by date or source to manage directory sizes. This layer is mainly for archival and for any future re-processing of raw videos. Downstream users (like model training jobs) can pull video files from here if needed. Cold storage (like tapes or Glacier) could be used for older videos if cost is a concern, but since storage is “unlimited” in the premise, we focus on accessibility.

- **Metadata Database (Structured):** We use a **relational or NoSQL database** to store structured metadata for each video. This includes:
  - Video ID (primary key), source info (URL, etc.), and any human-provided metadata from ingestion.
  - Technical metadata: duration, resolution, fps, codec, etc.
  - Generated labels: This can be stored as fields, for example: caption text, list of objects (perhaps as an array or as a separate join table for many-to-many), scene type, time of day, etc. 
  - We might use a relational DB like **PostgreSQL** or a scalable NoSQL like **Cassandra** depending on read/write patterns. Given 100M records, a distributed NoSQL or NewSQL (CockroachDB, etc.) might be suitable to handle high concurrency queries. Alternatively, an Elasticsearch cluster can serve as both a text search engine and a storage for metadata.
  - We ensure this DB is indexed appropriately for frequent queries. For example, full-text index on captions and tags for keyword searches, and secondary indexes on fields like “objects contains ‘cat’” or duration ranges. This allows users to quickly filter videos by textual criteria or attributes.

- **Vector Index (Semantic Search):** One of the outputs of labeling is embeddings – e.g., a video content embedding (from the final layer of a vision model) and possibly a text embedding (from the caption). We load these embeddings into a **vector search index** to enable semantic similarity search. A popular choice is **Faiss (Facebook AI Similarity Search)**, which is a library optimized for billion-scale vector similarity ([Faiss: A library for efficient similarity search - Engineering at Meta](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/#:~:text=This%20month%2C%20we%20released%20Facebook,dimensional%20vectors)). We can use Faiss to index the video embeddings (perhaps 100M vectors of dimension D). Using Faiss’s IVF or HNSW indexes, we can perform nearest-neighbor search in sub-second time even at this scale ([Faiss: A library for efficient similarity search - Engineering at Meta](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/#:~:text=allows%20us%20to%20quickly%20search,dimensional%20vectors)). This allows queries like “find videos that are visually similar to this query image or this example video.” For text embeddings (from captions), we could either use the same vector index or use a separate one – but often text search can be handled by Elasticsearch with BM25 or by vector search as well if we embed queries. Another modern solution is to use a dedicated **vector database** like **Weaviate or Milvus**. **Weaviate**, for instance, can store objects with both structured data and vector embeddings, enabling hybrid queries (combining keyword filters with similarity search) ([Introduction | Weaviate](https://weaviate.io/developers/weaviate/introduction#:~:text=Introduction%20,vector%20search%20with%20structured%20filtering)). Weaviate is scalable and could manage our 100M vectors partitioned over multiple nodes, offering a GraphQL or REST query interface for semantic search. The choice between Faiss (embedded in our app) vs Weaviate (as a service) depends on how we plan to query (Faiss is great for internal Python-based retrieval, Weaviate allows external API queries easily). Either way, this vector index is crucial for tasks like *“find duplicates”* (also used in dedup stage), *content-based retrieval*, or *recommendation systems* built on top of the dataset.

- **Indexing and Search Features:** For **full-text search** on captions and tags, we likely use **Elasticsearch** or **OpenSearch**. We can push each video’s metadata as a document into an ES cluster. The document might contain fields like caption (as text), objects (as keywords), etc. This allows complex text queries – e.g., “(dog OR cat) AND beach AND NOT night” – to retrieve relevant videos by boolean text match. Elasticsearch can also integrate with vector search plugins (for example, it has a kNN search feature for vectors). However, given we have specialized vector stores, we might keep text and vector search separate or use a unified solution like Weaviate that handles both. 

- **Data Lake for Analytics:** Optionally, we also store all metadata in a Parquet/CSV form on HDFS so that it can be analyzed with big data tools (Spark, Hive) for analytics beyond just search (e.g., analyzing distribution of labels, training new models on the metadata). This is more for maintainability.

Everything is tied together with a unique ID for each video clip that serves as the key across these systems (DB row, vector index entry, file path). When a user or downstream application queries the dataset, they might do a text search to narrow candidates, then a vector similarity search for ranking, etc. For example, a user query “a cat playing piano” could be converted to a text filter (caption contains cat and piano) on the DB and also to an embedding (using the same model that embedded videos) to do a vector search in the embedding index; the intersection yields the best results.

The **latency** for retrieval is kept low by these indexes. A well-tuned Elasticsearch cluster can search 100M short text records in under a second or two with proper indexing. Faiss can do 100M vector search in tens of milliseconds on GPU or a bit more on CPU with approximation. Weaviate and others are designed for sub-second responses at this scale by sharding the data.

The storage system is also designed for **scalability** – as more videos come in (beyond 100M), we can horizontally scale the DB and vector index by adding nodes. Partitioning by video ID range or by date can distribute load. The unlimited storage assumption means we don’t worry about space, but we do ensure backups and replication so that a node loss doesn’t lose data (metadata DB should be replicated, object storage with multiple copies).

Finally, we consider **cost**: storing 100M videos plus embeddings (which might be large if each is, say, a 512-D float vector). Embeddings for 100M items (512-d float) is ~200GB, which is fine. Text metadata is smaller. The videos themselves could be petabytes (depending on length/quality), but “unlimited storage” covers that. We might choose to store only a compressed version of videos if raw were too large.

## 7. Scalability and Maintainability  
Designing for 100M videos is just the start – we anticipate scaling further and updating the system as technology evolves. Key considerations for maintainability:

- **Modular Pipeline & Model Versioning:** Each stage of the pipeline (ingestion, dedup, labeling) is modular. We containerize each major component (e.g., a container for the object detection model, one for the captioning model). This allows us to **upgrade models or swap them out** without disrupting the whole system. For instance, if a new state-of-the-art action recognition model comes out, we can deploy it in place of the current one by updating that service’s container image. We use a model registry to keep track of versions (e.g., caption_model_v1, v2, etc.), and the metadata records include which model version labeled them. This versioning is important for dataset consistency and for future re-labeling if we improve models.

- **Retraining and Continuous Improvement:** With such a large dataset, we can continuously improve our AI models. The architecture supports **offline retraining**: we can take a sample of the labeled data (including those with human corrections) to fine-tune the captioning model or others. Because we store all outputs, we have a trove of weak labels that can serve as training data for multi-task models. We can also incorporate new data sources or ground-truth labels over time. The pipeline can be run in *re-processing mode* on existing videos if we want to update their labels with a new model version – thanks to unlimited storage, we kept raw videos, so we can re-run just the labeling stage on all videos with a new model if needed (though 100M is huge, this might be done gradually or on a subset that needs it).

- **Scaling Outward:** The system is built on distributed frameworks and Kubernetes, so scaling beyond 100M videos primarily means adding more compute and storage. If we double the dataset size, we can add more GPU nodes and CPU nodes; Ray and K8s make it straightforward to parallelize more. The architecture does not have hard bottlenecks that would cap at 100M – e.g., the databases chosen (Cassandra, Elastic, or Weaviate) can shard data and handle billions of records by adding nodes. The design avoids single points of failure by being distributed at every tier. We also ensure that adding new data (incremental updates) is possible: the pipeline can run continuously, ingesting new videos daily rather than a one-time batch.

- **Monitoring and Logging:** For maintainability, we set up extensive monitoring of the pipeline. This includes resource usage of each stage, throughput (videos processed per minute), queue backlogs, etc. Tools like Prometheus/Grafana can track these metrics. Alerts can be triggered if any stage falls behind or if error rates spike. Each video processing could produce warnings (say if a model couldn’t process a video). Logging each video’s processing status (success, error, duplicate, etc.) into a log or small DB helps in auditing the pipeline. If a crash or re-run is needed, we know where to resume.

- **Failure Recovery and Idempotence:** The pipeline tasks are designed to be **idempotent** where possible – e.g., if a process crashes after downloading and halfway labeling, it can resume without duplicating work (since we check if a video ID is already processed in the DB, etc.). Using message queues or Ray’s fault tolerance, we ensure no video is lost or processed twice even if workers fail. The metadata DB can have a flag for “completed” vs “in-process”. This way, maintainability also means operational reliability.

- **Future Expansion:** The system can incorporate **new modalities or tasks** in the future. For example, if we later want to add **subtitles OCR** (extracting any on-screen text) or **segmentation masks** for objects, we can add that as another labeling sub-stage with its own model. It would output additional metadata (like text found in video frames, or segmentation data stored perhaps as binary mask or annotation – though storing 100M masks might be heavy, so probably not unless needed). Because the pipeline is flexible, adding such a module is feasible. Similarly, if we integrate a new vector embedding model (say a future multimodal model that embeds audio+video together, like Meta’s **ImageBind** which creates a joint embedding for audio and visual), we can compute those embeddings for all videos and add them to the vector index to improve semantic search capabilities.

- **Documentation and Access:** For maintainability, thorough documentation of each component is provided (so engineers can understand the flow). Also, we might create an API or interface for researchers to query this dataset (the DB and vector search). Ensuring that this API is well-designed and stable means external users can build on the data without needing to know the internals. 

In summary, the architecture is built to **scale horizontally**, use **state-of-the-art models** with the ability to upgrade them, and handle the operations of such a massive pipeline robustly. By leveraging the latest in distributed computing and AI, this system can curate and index 100M video clips with minimal human labor, while producing rich metadata that makes the collection extremely searchable and useful for downstream AI applications.

## References and Sources

- NVIDIA NeMo Video Curator pipeline – streaming multi-stage video processing with Ray, balancing CPU/GPU usage ([Petabyte-Scale Video Processing with NVIDIA NeMo Curator on NVIDIA DGX Cloud | NVIDIA Technical Blog](https://developer.nvidia.com/blog/petabyte-scale-video-processing-with-nvidia-nemo-curator-on-nvidia-dgx-cloud/#:~:text=For%20each%20stage%2C%20the%20main,with%20the%20actor%20pool%20size)) ([Petabyte-Scale Video Processing with NVIDIA NeMo Curator on NVIDIA DGX Cloud | NVIDIA Technical Blog](https://developer.nvidia.com/blog/petabyte-scale-video-processing-with-nvidia-nemo-curator-on-nvidia-dgx-cloud/#:~:text=Based%20on%20the%20relative%20throughput%2C,is%20fluctuation%20in%20the%20pipeline)). This provided insight into dividing tasks (decode, filtering, annotation, embedding) and auto-scaling resources per stage.  
- Anyscale/Ray & NVIDIA example – processing ~1 million hours of video on 2000 GPUs/day, highlighting the need for heterogeneous pipeline concurrency ([Nvidia just released a large-scale video curation pipeline at GTC built… | Anyscale](https://www.linkedin.com/posts/joinanyscale_nvidia-just-released-a-large-scale-video-activity-7308169936755933184-neGq#:~:text=Ray,in%2FeCe37rsG)).  
- AWS Media Blog on AI Deduplication – using frame-level video and audio embeddings to detect duplicates, even with minor differences (e.g., watermarks) ([Using computer vision to automate media content deduplication workflows | AWS for M&E Blog](https://aws.amazon.com/blogs/media/using-computer-vision-to-automate-media-content-deduplication-workflows/#:~:text=Keeping%20the%20above%20in%20mind%2C,secure%20as%20our%20customers%20scale)) ([Using computer vision to automate media content deduplication workflows | AWS for M&E Blog](https://aws.amazon.com/blogs/media/using-computer-vision-to-automate-media-content-deduplication-workflows/#:~:text=embeddings%20for%20the%20audio%20and,contains%20essential%20information%20such%20as)). This shows the effectiveness of embedding-based duplicate detection with high granularity.  
- MPEG-7 Video Signature standard – robust content fingerprints for identifying duplicate or partial-match videos in large databases ([Mpeg 7 video signature tools for content recognition | PPT](https://www.slideshare.net/slideshow/mpeg-7-video-signature-tools-for-content-recognition/26134618#:~:text=The%20document%20discusses%20the%20MPEG,Read%20less)) ([Mpeg 7 video signature tools for content recognition | PPT](https://www.slideshare.net/slideshow/mpeg-7-video-signature-tools-for-content-recognition/26134618#:~:text=Image%3A%20Why%20The%20MPEG,longer%20piece%20of%20video%20content)), enabling detection of clips embedded in longer videos.  
- Faiss (Facebook AI Similarity Search) – vector indexing library for billion-scale similarity search, enabling fast search over high-dimensional embeddings ([Faiss: A library for efficient similarity search - Engineering at Meta](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/#:~:text=This%20month%2C%20we%20released%20Facebook,dimensional%20vectors)).  
- Weaviate vector database – an open-source vector DB that supports semantic search and hybrid queries (combining vector similarity with filters) ([Introduction | Weaviate](https://weaviate.io/developers/weaviate/introduction#:~:text=Introduction%20,vector%20search%20with%20structured%20filtering)), useful for our metadata+embedding search needs.  
- SlowFast action recognition model (Facebook AI Research) – an example of a state-of-the-art video action classifier using a two-stream CNN for spatial and temporal features ([SlowFast Networks for Video Recognition](https://openaccess.thecvf.com/content_ICCV_2019/papers/Feichtenhofer_SlowFast_Networks_for_Video_Recognition_ICCV_2019_paper.pdf#:~:text=We%20present%20SlowFast%20networks%20for,pointed)).  
- Google Research Vid2Seq – a modern vision-language transformer model for video captioning, trained on millions of videos, demonstrating improved captioning of complex events ([Vid2Seq: a pretrained visual language model for describing multi-event videos](https://research.google/blog/vid2seq-a-pretrained-visual-language-model-for-describing-multi-event-videos/#:~:text=model%20ai,video%20paragraph%20captioning%20task%2C%20and)) ([Vid2Seq: a pretrained visual language model for describing multi-event videos](https://research.google/blog/vid2seq-a-pretrained-visual-language-model-for-describing-multi-event-videos/#:~:text=A%20visual%20language%20model%20for,dense%20video%20captioning)).  
- AudioSet ontology – Google’s dataset of 2 million videos with 632 audio event classes, and the VGGish model for embedding audio, which informs our audio tagging approach ([AudioSet - A sound vocabulary and dataset - Google Research](https://research.google.com/audioset/#:~:text=Research%20research,second)) ([models/research/audioset/vggish/README.md at master - GitHub](https://github.com/tensorflow/models/blob/master/research/audioset/vggish/README.md#:~:text=VGGish%20converts%20audio%20input%20features,to%20a%20downstream%20classification%20model)).  

These sources and tools inform the design choices in our pipeline, ensuring we use **proven, up-to-date techniques** for scalable video data processing and annotation. The end result is a system capable of curating an enormous video dataset with rich, searchable metadata – enabling advanced analytics and model training on video content at an unprecedented scale.